---
title: 'Loss Functions for Logistic Regression'
date: 2022-02-24
permalink: /posts/2022/02/blog-post-1/
tags:
  - ML Basic
---

Why Mean Squared Error (MSE) is Not Suitable for Logistic Regression
======

##  Introduction  
When training a machine learning model, choosing the right **loss function** is critical to ensuring effective learning. In **linear regression**, **Mean Squared Error (MSE)** is commonly used, but for **logistic regression**, it becomes problematic.  

In this post, we’ll explore **why MSE is not suitable for logistic regression** and why **cross-entropy loss (log-loss) is the better choice**.  

---

##  Understanding Logistic Regression  
Logistic regression is a **classification algorithm** used to predict **binary outcomes** (0 or 1). Instead of modeling a continuous output like linear regression, it **models probabilities** using the **sigmoid activation function**:

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

where:  
- \\(\hat{y} \\} is the predicted probability.  
- \\( z = Xw + b \\) is the linear combination of inputs and weights.  
- \\( \sigma(z) \\) (sigmoid function) ensures the output is in the range \\([0,1]\\).  

Since logistic regression is used for classification, our goal is to **maximize the likelihood** of correctly predicting class labels.

---

## Why Not Use MSE for Logistic Regression?  
MSE is defined as:

$$
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

However, using MSE in logistic regression causes **three major problems**:

### 1. MSE Leads to a Non-Convex Loss Function  
- The **sigmoid function introduces non-linearity**, and when combined with MSE, the loss function becomes **non-convex**.  
- **Gradient descent struggles** to converge efficiently because it can get stuck in **local minima**.  
- **Cross-entropy loss is convex**, ensuring **faster and more stable convergence**.

###  2. MSE Treats Probabilities as Continuous Values  
- Logistic regression models **probabilities**, but **MSE assumes continuous errors**, which is **not optimal for classification**.  
- **Example:** If the actual label is **1**, and the model predicts **0.9**, MSE gives a **small loss**, but in classification, we need a **sharper distinction**.

### 3. MSE Leads to Small Gradients (Slow Learning)  
The **gradient of MSE** with respect to weights:

$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial w} = (y - \hat{y}) \cdot \hat{y} \cdot (1 - \hat{y}) \cdot X
$$

When \\(\hat{y}\\) is **close to 0 or 1**, the term \\(\hat{y} (1 - \hat{y})\\) becomes **very small**, leading to **vanishing gradients**.
- **This slows down learning** and makes convergence inefficient.

---

##  Why Cross-Entropy Loss is Better  
Instead of MSE, logistic regression uses **cross-entropy loss**:

$$
\mathcal{L}_{\text{CE}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

### ✅ **Key Benefits of Cross-Entropy Loss:**  
✔ **Convex loss function** → Ensures **faster & more stable convergence**.  
✔ **Better probability modeling** → Matches the **Bernoulli distribution**.  
✔ **Prevents small gradient issues** → Ensures **meaningful weight updates**.

---

##  Visualization: Comparing MSE vs. Cross-Entropy Loss  
To better understand the difference, let's visualize how **MSE vs. Cross-Entropy Loss** behave for classification.

```python
import numpy as np
import matplotlib.pyplot as plt

# Define sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Define loss functions
def mse_loss(y_true, y_pred):
    return (y_true - y_pred) ** 2

def cross_entropy_loss(y_true, y_pred):
    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Generate predictions
y_true = 1  # Positive class
z_values = np.linspace(-5, 5, 100)
y_preds = sigmoid(z_values)

# Compute loss values
mse_losses = mse_loss(y_true, y_preds)
ce_losses = cross_entropy_loss(y_true, y_preds)

# Plot the losses
plt.figure(figsize=(8, 5))
plt.plot(y_preds, mse_losses, label="MSE Loss", linestyle='dashed')
plt.plot(y_preds, ce_losses, label="Cross-Entropy Loss", linestyle='solid')
plt.xlabel("Predicted Probability")
plt.ylabel("Loss")
plt.title("Comparison of MSE vs. Cross-Entropy Loss")
plt.legend()
plt.grid()
plt.show()