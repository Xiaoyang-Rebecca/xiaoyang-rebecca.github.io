---
title: 'From Text Transfromer models to Vision Transformer'
date: 2025-01-10
permalink: /posts/2025/01/blog-post-1/
tags:
  - ML Basic
---
# Understanding Transformer Models in Text and Vision AI

## **Introduction**
Transformer models have revolutionized **large language models (LLMs)** and are now widely used across **multimodal AI applications**, including **text generation, conversational AI, and vision-based models**.

This post explores the **core components** of the **Transformer architecture** in **text generation** and how it extends to **Vision Transformers (ViTs)**.

---

## **1ï¸âƒ£ Transformer Architecture for Text Generation**
The **Transformer model** consists of several key components that enable efficient **contextual understanding** and **text generation**.  

### **1.1 Text Tokenization and Encoding**
- **Process:** Converts text into **fixed-length vectors (embeddings)**.
- **Purpose:** Groups words with similar meanings **closer in vector space** rather than relying on exact word matching.

ğŸ“Œ **Example:**
- **"cat"** and **"car"** may be close in spelling but are **semantically distant**.
- **"cat"** and **"dog"** have similar **attributes (pets)**, so their embeddings **are closer**.

ğŸ’¡ **Extension:** Text encoding is also crucial in **search and recommendation systems**, where **pre-trained embeddings** can improve **retrieval quality**.

---

### **1.2 Positional Encoding**
Transformers do not have an inherent understanding of word order.  
To **preserve positional information**, they use a **sinusoidal function**:

\[
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\]

\[
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
\]

ğŸ“Œ **Example:**
- **"The cat lies on the sofa"** vs **"The sofa lies on the cat"**  
- Without positional encoding, the model might treat them **as identical**.
- **Output:** Final embedding = **Text embedding + Positional encoding**.

ğŸ”¹ **Takeaway:** Positional encoding ensures **word order matters** in Transformer models.

---

### **1.3 Attention Mechanism**
The **attention mechanism** measures **importance** between words, enabling the model to **focus on crucial elements** for making predictions.

#### **Attention Formula:**
\[
\text{Attention}(Q, K, V) = \text{Softmax} \left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

| **Component** | **Definition** |
|--------------|---------------|
| **Q (Query)** | Input embedding |
| **K (Key)** | Contextual meaning |
| **V (Value)** | Actual token information |

ğŸ”¹ **Takeaway:** Attention **helps models understand dependencies** between words, leading to more **coherent text generation**.

---

### **1.4 Self-Attention**
- Measures **word relationships** within a sentence.  
- Helps maintain **context and coherence**.

ğŸ“Œ **Example:**  
Sentence: **"The dog chased the cat because it was fast."**  
- **Who is "it"?**  
- Self-attention allows the model to **understand that "it" refers to "cat"** based on prior context.

| **Q** | **K** | **V** |
|-------|-------|-------|
| Word embedding (512-dim) | Word itself | Word embedding |

ğŸ”¹ **Takeaway:** **Self-attention improves contextual accuracy**.

---

### **1.5 Multi-Head Attention**
- **Enhances long-sequence learning** by running **multiple attention heads in parallel**.
- **Divides embeddings into smaller subspaces**, improving efficiency.

ğŸ“Œ **Example:**  
- **512-dimensional embedding** â†’ Split into **8 attention heads** â†’ Each head gets **64 dimensions**.
- This allows **different heads to focus on different linguistic aspects**.

ğŸ”¹ **Takeaway:** **Multi-head attention allows parallel processing**, making **Transformer models more efficient**.

---

### **1.6 Cross-Attention**
- Measures **importance between different sets of data**.
- **Common in translation & multimodal tasks**.

ğŸ“Œ **Example: Translation Task**
| **Q** (Target sentence) | **K** (Source text) | **V** (Source embeddings) |
|-------------------------|---------------------|--------------------------|
| "La maison est grande" | "The house is big" | Encoded vector of source text |

- **Used in RAG (Retrieval-Augmented Generation)** to fetch **external knowledge**.
- **Used in multi-turn dialogue** to maintain **conversation flow**.

ğŸ”¹ **Takeaway:** **Cross-attention integrates external context**, improving LLM performance.

---

### **1.7 Applications of QKV in Conversational AI**
| **Application** | **Query (Q)** | **Key (K)** | **Value (V)** |
|----------------|--------------|-------------|---------------|
| **Self-Attention (Single Turn Understanding)** | Current word embedding | Other word embeddings | Contextualized representation |
| **Cross-Attention (Multi-Turn Dialogue)** | User input | Previous conversation history | Relevant responses |
| **Cross-Attention (Knowledge Retrieval - RAG)** | User question | Knowledge base chunks | Factual information |
| **Cross-Attention (Task-Oriented Chatbot)** | User request | API functions | Retrieved structured data |

---

## **2ï¸âƒ£ Vision Transformers (ViT)**
Vision Transformers extend **Transformer models** from text to **images**.

### **2.1 Image Embedding**
- Instead of **text tokenization**, ViTs use **image patch embeddings**.
- **Flatten 2D pixel patches** into **1D vector representations**.

ğŸ“Œ **Example:**  
- Image = **256Ã—256 pixels**  
- Divide into **16Ã—16 patches** â†’ **Flatten** into vectors.

---

### **2.2 Positional Encoding in Images**
- Unlike text, images need **2D positional encoding**.
- Each **tile's position is assigned coordinates** like:
  \[
  (0,0), (0,1), (1,0), (1,1)
  \]
- Example: **Top-left, top-right, bottom-left, bottom-right**.

ğŸ”¹ **Takeaway:** **2D positional encoding ensures spatial relationships are preserved** in Vision Transformers.

---

## **ğŸš€ Summary Table: Transformers for Text vs. Vision**
| **Component** | **Text Transformers** | **Vision Transformers (ViT)** |
|--------------|----------------------|----------------------------|
| **Embedding** | Token embeddings (e.g., 512-dim) | Patch embeddings (flattened 2D image) |
| **Positional Encoding** | 1D Sin/Cos function | 2D encoding for spatial positions |
| **Attention Mechanism** | Focuses on important words | Focuses on important image patches |
| **Cross-Attention** | Used in translation & retrieval tasks | Used for multimodal learning (image-text alignment) |

---

## **ğŸš€ Final Takeaways**
âœ” **Transformers power LLMs & multimodal AI**, enabling **efficient learning across text & images**.  
âœ” **Self-attention enhances sentence-level understanding**, while **cross-attention integrates external knowledge**.  
âœ” **Vision Transformers modify tokenization & encoding**, adapting Transformers for **image-based tasks**.

---

## **ğŸ”¥ Why This Revision Works**
âœ… **Fact-Checked Formulas** â€“ Ensured correctness of **positional encoding and attention** equations.  
âœ… **Markdown Formatting** â€“ Improved readability for **publishing on GitHub, Medium, or personal blogs**.  
âœ… **Clearer Examples & Explanations** â€“ Added **real-world use cases** to enhance understanding.  

ğŸš€ **Ready to publish!** Would you like any **final refinements** before posting? ğŸ˜Š